{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa2f862",
   "metadata": {},
   "source": [
    "# Playground Notebook\n",
    "\n",
    "This notebook is meant to be temporarily used to experiment with different parts of the code contained in the project regarding document parsing and extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d17c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "import pymupdf4llm\n",
    "from azure.storage.blob import BlobClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Local Imports from your existing codebase\n",
    "from shared.extractors import ExtractorFactory, QuestionExtractorConfig, QAExtractorConfig\n",
    "from shared.extractors.base import SectionType\n",
    "from shared.extractors.base.models import ExtractedQAPair\n",
    "from shared.parsers.pymupdf_parser import PyMuPDFParser\n",
    "from shared.parsers.azure_parser import AzureDocumentParser\n",
    "from shared.llm import create_llm_client\n",
    "from config import config\n",
    "\n",
    "\n",
    "factory = ExtractorFactory()\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fa1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This utility function can be used to get the PDF bytes\n",
    "# from a PDF on a blob storage given a blob URL.\n",
    "\n",
    "# This can also be done with local PDFs and open source\n",
    "# python packages, which was done throughout recent developments\n",
    "# to minimize costs.\n",
    "\n",
    "def get_blob_content(blob_url: str) -> bytes:\n",
    "    \"\"\"Downloads the blob content into memory.\"\"\"\n",
    "    logging.info(f\"Connecting to Blob: {blob_url.split('?')[0]}...\") # Log URL without SAS token\n",
    "    \n",
    "    try:\n",
    "        # Try connecting with Default Credential (env vars or az login)\n",
    "        # If the URL contains a SAS token, the credential is ignored automatically by the SDK\n",
    "        blob_client = BlobClient.from_blob_url(\n",
    "            blob_url, \n",
    "            credential=DefaultAzureCredential()\n",
    "        )\n",
    "        \n",
    "        download_stream = blob_client.download_blob()\n",
    "        return download_stream.readall()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download blob: {e}\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9366c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are very crude functions to calculate and display metrics\n",
    "# related to the extraction. Since they might not be included\n",
    "# in a production environment, I'm leaving them here.\n",
    "\n",
    "\n",
    "def levenshtein_distance(s1: str, s2: str) -> int:\n",
    "    \"\"\"Compute Levenshtein distance between two strings.\"\"\"\n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "\n",
    "    if len(s1) < len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "\n",
    "    for i, c1 in enumerate(s1, start=1):\n",
    "        current_row = [i]\n",
    "        for j, c2 in enumerate(s2, start=1):\n",
    "            insertions = previous_row[j] + 1\n",
    "            deletions = current_row[j - 1] + 1\n",
    "            substitutions = previous_row[j - 1] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "\n",
    "def levenshtein_similarity(s1: str, s2: str) -> float:\n",
    "    \"\"\"\n",
    "    Normalize Levenshtein distance into a similarity score [0, 1].\n",
    "    1.0 = exact match\n",
    "    \"\"\"\n",
    "    distance = levenshtein_distance(s1, s2)\n",
    "    max_len = max(len(s1), len(s2))\n",
    "    return 1 - distance / max_len if max_len > 0 else 1.0\n",
    "\n",
    "\n",
    "def max_similarity_per_question(questions_dict, questions_ground_truth_2):\n",
    "    results = {}\n",
    "\n",
    "    for q_key, q_text in questions_dict.items():\n",
    "        best_match = None\n",
    "        best_score = 0.0\n",
    "\n",
    "        for gt_key, gt_text in questions_ground_truth_2.items():\n",
    "            score = levenshtein_similarity(q_text, gt_text)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = gt_key\n",
    "\n",
    "        results[q_key] = {\n",
    "            \"best_ground_truth_question\": best_match,\n",
    "            \"max_similarity\": round(best_score, 4)\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def count_accuracy(extracted_items, ground_truth_dict):\n",
    "    y_hat = len(extracted_items)\n",
    "    y = len(ground_truth_dict)\n",
    "    # acc = 1 - (abs(y_hat - y)/y)\n",
    "    acc = 1 - (y_hat - y)/y\n",
    "    return acc\n",
    "\n",
    "\n",
    "def content_similarity(extracted_items, ground_truth_items):\n",
    "    similarity_list = []\n",
    "    for y_hat in extracted_items:\n",
    "        similarity = max([levenshtein_similarity(y_hat, y) for y in ground_truth_items])\n",
    "        similarity_list.append(similarity)\n",
    "    return similarity_list\n",
    "\n",
    "\n",
    "def display_metrics(extractions, ground_truth, structure=None):\n",
    "\n",
    "    # Questions\n",
    "    y_questions = [q['question'] for q in ground_truth['questions_answers']]\n",
    "    yhat_questions = [q.question_text for q in extractions]\n",
    "    q_similarity = content_similarity(yhat_questions, y_questions)\n",
    "    print(f\"Question (or pair) count accuracy: {count_accuracy(yhat_questions, y_questions):.2f}\")\n",
    "    print(f\"Questions similarity: Average={np.mean(q_similarity):.2f} Min={np.min(q_similarity):.2f} Max={np.max(q_similarity):.2f}\")\n",
    "\n",
    "    # Answers\n",
    "    if type(extractions[0]) == ExtractedQAPair:\n",
    "        y_answers = [q['answer'] for q in ground_truth['questions_answers']]\n",
    "        yhat_answers = [q.answer_text for q in extractions]\n",
    "        yhat_answers = [text if text != None else \"\" for text in yhat_answers]\n",
    "        a_similarity = content_similarity(yhat_answers, y_answers)\n",
    "        print(f\"Answers similarity: Average={np.mean(a_similarity):.2f} Min={np.min(a_similarity):.2f} Max={np.max(a_similarity):.2f}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No answers to be evaluated\")\n",
    "\n",
    "    # Section\n",
    "    if structure != None:\n",
    "        y_sections = [q['title'] for q in ground_truth['sections']]\n",
    "        yhat_sections = [q.section_title for q in structure.sections]\n",
    "        s_similarity = content_similarity(yhat_sections, y_sections)\n",
    "        print(f\"Section count accuracy: {count_accuracy(yhat_sections, y_sections):.2f}\")\n",
    "        print(f\"Section similarity: Average={np.mean(s_similarity):.2f} Min={np.min(s_similarity):.2f} Max={np.max(s_similarity):.2f}\")\n",
    "    else:\n",
    "        print(\"No document structure to be evaluated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119462d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truths of different documents to quantitatively\n",
    "# estimate the extraction performance\n",
    "\n",
    "with open(\"/home/dougsgrott-wsl/projects/local-doc-extractor/data/ground_truth/rfp_email.json\", \"r\") as f:\n",
    "    questions_ground_truth_esp = json.load(f)\n",
    "\n",
    "with open(\"/home/dougsgrott-wsl/projects/local-doc-extractor/data/ground_truth/rfp_market.json\", \"r\") as f:\n",
    "    questions_ground_truth_ip = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c441c199",
   "metadata": {},
   "source": [
    "## Document Loading and Parsing\n",
    "Ultimately, the PDF documents will be loaded using Azure Blob, and the text parsing can be done with either Azure Document Intelligence (DI) or some python open source package.\n",
    "\n",
    "As an alternative to DI, an open source parser is provided using pymupdf. I suggest that the decision to use it instead of DI should be based on metrics of Q/Q&A extraction on real RFPs.\n",
    "\n",
    "Pros of Azure Document Intelligence:\n",
    "- Can be used for multiple file types (pdf, docx, etc)\n",
    "- If you build the text from the paragraphs directly instead of accessing the content key, you can get the table content in markdown style.\n",
    "- Every extracted text is accompanied with a bounding box representing in its location on the document.\n",
    "  - This can be useful to filter out headers and footers, diminishing OCR noise.\n",
    "- The service tries to extract role of each element (paragraph, header, section, etc).\n",
    "  - This could be useful to programatically reconstruct the document structure without AI. The bad news is: it is pretty bad, and fixing broken structures programatically can be more complex than parsing it with LLM.\n",
    "\n",
    "Cons of Azure Document Intelligence:\n",
    "- It's very, very costly (in my experimentations, it represented 65~75% of the costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d5390e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_filepath_ = \"/home/dougsgrott-wsl/projects/local-doc-extractor/data/RFP - Email Service Provider.pdf\"\n",
    "doc = fitz.open(pdf_filepath_)\n",
    "pdf_bytes = doc.tobytes()\n",
    "\n",
    "free_parser = PyMuPDFParser()\n",
    "free_parsed_doc = free_parser.parse(file_content=pdf_bytes)\n",
    "\n",
    "# Should require Doc Int. endpoints configured\n",
    "# azure_parser = AzureDocumentParser()\n",
    "# azure_parsed_doc = azure_parser.parse(file_content=pdf_bytes)\n",
    "\n",
    "\n",
    "# Parser to be used in the content extraction section\n",
    "parser = free_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb747c91",
   "metadata": {},
   "source": [
    "# Content Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c019e",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec9ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "RFP_TYPE = \"ESP\" # \"ESP\" or \"PM\"\n",
    "\n",
    "if RFP_TYPE == \"ESP\":\n",
    "    # Email Service Provider (with answers)\n",
    "    url = \"/home/dougsgrott-wsl/projects/local-doc-extractor/data/RFP - Email Service Provider.pdf\"\n",
    "    questions_ground_truth = questions_ground_truth_esp\n",
    "elif RFP_TYPE == \"PM\":\n",
    "    # Public Market, template N (with answers)\n",
    "    url = f\"https://stdocpipelinedev24o1.blob.core.windows.net/input-pdfs/Public Market - Q template {N}.pdf\"\n",
    "    questions_ground_truth = questions_ground_truth_ip\n",
    "\n",
    "doc = parser.parse(file_content=pdf_bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69381df",
   "metadata": {},
   "source": [
    "### Simple extractor (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dde6de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 13:11:02,673 - INFO - Starting simple LLM question extraction...\n",
      "2025-12-29 13:11:02,675 - INFO - Processing 6 chunks...\n",
      "2025-12-29 13:11:10,251 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:11:19,017 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:11:27,055 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:11:37,090 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:11:47,037 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:11:50,297 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:11:50,524 - INFO - Extracted 40 questions using simple LLM approach\n"
     ]
    }
   ],
   "source": [
    "config = QuestionExtractorConfig(allowed_section_types=[SectionType.QUESTIONNAIRE])\n",
    "question_extractor = factory.create_question_extractor(\"simple_llm\")\n",
    "questions_simple = question_extractor.extract(doc.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "476a1bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question (or pair) count accuracy: 0.75\n",
      "Questions similarity: Average=0.59 Min=0.26 Max=1.00\n",
      "No answers to be evaluated\n",
      "No document structure to be evaluated\n"
     ]
    }
   ],
   "source": [
    "display_metrics(questions_simple, questions_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e05c3",
   "metadata": {},
   "source": [
    "### Extractor with bells and whistles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc5a153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 13:11:54,416 - INFO - Detecting document structure...\n",
      "2025-12-29 13:11:54,419 - INFO - Starting AI-powered structure detection...\n",
      "2025-12-29 13:11:54,422 - INFO - Created 5 strategic samples\n",
      "2025-12-29 13:11:58,915 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:11:59,142 - INFO - Format detected: formal_numbered (confidence: 0.95)\n",
      "2025-12-29 13:13:13,697 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:13:14,012 - INFO - Detected 32 sections\n",
      "2025-12-29 13:13:14,018 - INFO - Structure detection complete. Sections: 30\n",
      "2025-12-29 13:13:14,019 - INFO - Extracting with structure awareness (30 sections)\n",
      "2025-12-29 13:13:39,442 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:14:00,050 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:14:00,062 - INFO - After deduplication: 34 questions\n"
     ]
    }
   ],
   "source": [
    "config = QuestionExtractorConfig(allowed_section_types=[SectionType.QUESTIONNAIRE])\n",
    "structure_extractor = factory.create_structure_extractor(\"structure_aware\")\n",
    "question_extractor = factory.create_question_extractor(\"context_aware\", config)\n",
    "question_extractor.set_structure_extractor(structure_extractor)\n",
    "\n",
    "questions_aware = question_extractor.extract(doc.content)\n",
    "structure = question_extractor.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a76c0111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question (or pair) count accuracy: 0.94\n",
      "Questions similarity: Average=0.81 Min=0.21 Max=1.00\n",
      "No answers to be evaluated\n",
      "Section count accuracy: -1.00\n",
      "Section similarity: Average=0.52 Min=0.19 Max=1.00\n"
     ]
    }
   ],
   "source": [
    "display_metrics(questions_aware, questions_ground_truth, structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffc677",
   "metadata": {},
   "source": [
    "## Question and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1e0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "RFP_TYPE = \"ESP\" # \"ESP\" or \"PM\"\n",
    "\n",
    "if RFP_TYPE == \"ESP\":\n",
    "    # Email Service Provider (with answers)\n",
    "    url = \"https://stdocpipelinedev24o1.blob.core.windows.net/input-pdfs/RFP - Email Service Provider (with answers) v5.pdf\"\n",
    "    questions_ground_truth = questions_ground_truth_esp\n",
    "elif RFP_TYPE == \"PM\":\n",
    "    # Public Market, template N (with answers)\n",
    "    url = f\"https://stdocpipelinedev24o1.blob.core.windows.net/input-pdfs/Public Market - QA Template {N}.pdf\"\n",
    "    questions_ground_truth = questions_ground_truth_ip\n",
    "\n",
    "doc = parser.parse(file_content=pdf_bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce4413",
   "metadata": {},
   "source": [
    "### Simple extractor (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95564f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 13:16:04,701 - INFO - Split document into 5 chunks for extraction (chunk_size=8000, overlap=500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 13:16:25,431 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:16:38,767 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:16:52,318 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:16:59,228 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:17:00,042 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:17:00,252 - INFO - Extracted 38 Q&A pairs (before deduplication)\n",
      "2025-12-29 13:17:00,264 - INFO - After deduplication: 37 Q&A pairs\n"
     ]
    }
   ],
   "source": [
    "config = QAExtractorConfig(allowed_section_types=[SectionType.QUESTIONNAIRE])\n",
    "question_extractor = factory.create_qa_extractor(\"simple\")\n",
    "questions_simple = question_extractor.extract(doc.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746f5ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question (or pair) count accuracy: 0.84\n",
      "Questions similarity: Average=0.48 Min=0.26 Max=1.00\n",
      "Answers similarity: Average=0.98 Min=0.25 Max=1.00\n",
      "No document structure to be evaluated\n"
     ]
    }
   ],
   "source": [
    "display_metrics(questions_simple, questions_ground_truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7791f0aa",
   "metadata": {},
   "source": [
    "### Extractor with bells and whistles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5abfe995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 13:17:04,568 - INFO - Detecting document structure...\n",
      "2025-12-29 13:17:04,569 - INFO - Starting AI-powered structure detection...\n",
      "2025-12-29 13:17:04,571 - INFO - Created 5 strategic samples\n",
      "2025-12-29 13:17:08,874 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:17:09,092 - INFO - Format detected: formal_numbered (confidence: 0.95)\n",
      "2025-12-29 13:18:23,328 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:18:23,544 - INFO - Detected 32 sections\n",
      "2025-12-29 13:18:23,547 - INFO - Structure detection complete. Sections: 30\n",
      "2025-12-29 13:18:23,547 - INFO - Extracting with structure awareness (30 sections)\n",
      "2025-12-29 13:18:39,924 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:18:56,135 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:18:56,144 - INFO - After deduplication: 34 questions\n"
     ]
    }
   ],
   "source": [
    "config = QuestionExtractorConfig(allowed_section_types=[SectionType.QUESTIONNAIRE])\n",
    "structure_extractor = factory.create_structure_extractor(\"structure_aware\")\n",
    "question_extractor = factory.create_question_extractor(\"context_aware\", config)\n",
    "question_extractor.set_structure_extractor(structure_extractor)\n",
    "\n",
    "questions_aware = question_extractor.extract(doc.content)\n",
    "structure = question_extractor.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b2ffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question (or pair) count accuracy: 0.94\n",
      "Questions similarity: Average=0.81 Min=0.21 Max=1.00\n",
      "No answers to be evaluated\n",
      "Section count accuracy: -1.00\n",
      "Section similarity: Average=0.52 Min=0.19 Max=1.00\n"
     ]
    }
   ],
   "source": [
    "display_metrics(questions_aware, questions_ground_truth_esp, structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36be97",
   "metadata": {},
   "source": [
    "# Triggerless End-to-End Pipeline\n",
    "\n",
    "This combines document loading, text parsing, content extraction and persistence/database.\n",
    "\n",
    "The code is/should be similar to the one inside the function_app:\n",
    " - Here, we can test loading the configuration and executing the code similarly as to how it will run on Azure, including text parsing (using Pymu of DI), Question/Q&A extraction (using OpenAI or Azure OpenAI) and saving to postgres.\n",
    " - However, instead of triggering the code when a PDF is uploaded, we simply run it whenever we want, loading the PDF bytes using Azure Blob or Pymupdf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db0af8",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "656b5fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 13:19:00,067 - INFO - [Questions] Using extraction strategy: context_aware\n",
      "2025-12-29 13:19:00,072 - INFO - [Questions] Structure extractor injected for context-aware extraction\n",
      "2025-12-29 13:19:00,073 - INFO - Processing file: myblob.name\n",
      "2025-12-29 13:19:00,073 - INFO - Step 1: Extracting text from PDF...\n",
      "2025-12-29 13:19:02,064 - INFO - Extracted 30023 characters of text (format: markdown)\n",
      "2025-12-29 13:19:02,065 - INFO - Step 2: Identifying Questions and Answers with LLM...\n",
      "2025-12-29 13:19:02,065 - INFO - Detecting document structure...\n",
      "2025-12-29 13:19:02,066 - INFO - Starting AI-powered structure detection...\n",
      "2025-12-29 13:19:02,066 - INFO - Created 5 strategic samples\n",
      "2025-12-29 13:19:04,415 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:19:04,635 - INFO - Format detected: formal_numbered (confidence: 0.95)\n",
      "2025-12-29 13:20:13,069 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:20:13,412 - INFO - Detected 32 sections\n",
      "2025-12-29 13:20:13,414 - INFO - Structure detection complete. Sections: 30\n",
      "2025-12-29 13:20:13,415 - INFO - Extracting with structure awareness (30 sections)\n",
      "2025-12-29 13:20:34,667 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:20:40,143 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:20:40,153 - INFO - After deduplication: 33 questions\n",
      "2025-12-29 13:20:40,153 - INFO - Extracted 33 potential Q&A pairs\n",
      "2025-12-29 13:20:40,154 - INFO - Step 3: Persisting to PostgreSQL...\n",
      "2025-12-29 13:20:40,728 - INFO - Successfully committed 33 Q&A pairs to DB\n",
      "2025-12-29 13:20:40,729 - INFO - [Questions] Success: Successfully processed 33 Q&A pairs\n"
     ]
    }
   ],
   "source": [
    "import azure.functions as func\n",
    "import logging\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session\n",
    "import fitz\n",
    "\n",
    "from config import config\n",
    "from shared.extractors import ExtractorFactory\n",
    "from shared.extractors.base.config import QuestionExtractorConfig\n",
    "from shared.extractors.base.models import ExtractedQAPair\n",
    "from shared.parsers import create_parser\n",
    "from shared.models.db import RFP\n",
    "from shared.models.enums import ProcessingStatus, RFPStatus, SourceType\n",
    "from shared.models.converters import bulk_extracted_qa_to_answers\n",
    "from shared.models.db.relationships import bulk_create_answers_with_rfp_update\n",
    "from shared.extractors.base import QuestionToQAAdapter\n",
    "from shared.core.processor import DocumentProcessor\n",
    "\n",
    "\n",
    "# Database Connection Setup\n",
    "engine = create_engine(config.database_url)\n",
    "\n",
    "# Extraction Strategy Configuration\n",
    "EXTRACTION_STRATEGY = os.getenv(\"EXTRACTION_STRATEGY\", \"context_aware\")\n",
    "# Options: simple_llm (fast, basic), context_aware (better quality, section-aware)\n",
    "\n",
    "\n",
    "def load_config() -> QuestionExtractorConfig:\n",
    "    \"\"\"Load question extraction configuration from environment variables.\"\"\"\n",
    "    return QuestionExtractorConfig(\n",
    "        chunk_size=int(os.getenv(\"CHUNK_SIZE\", \"6000\")),\n",
    "        overlap=int(os.getenv(\"CHUNK_OVERLAP\", \"500\")),\n",
    "        confidence_threshold=float(os.getenv(\"CONFIDENCE_THRESHOLD\", \"0.6\")),\n",
    "        deduplicate_questions=os.getenv(\"ENABLE_DEDUP\", \"true\").lower() == \"true\",\n",
    "        similarity_threshold=float(os.getenv(\"SIMILARITY_THRESHOLD\", \"0.9\")),\n",
    "        # LLM config auto-loaded from base ExtractorConfig\n",
    "    )\n",
    "\n",
    "\n",
    "def main(file_bytes):\n",
    "    \"\"\"\n",
    "    1. Parses the PDF document (using Azure DI or pymupdf)\n",
    "    2. Extracts questions only using LLM (no answer extraction)\n",
    "    3. Saves questions to the database with answer_text=NULL\n",
    "    \"\"\"\n",
    "    # logging.info(f\"[Questions] Processing file: {myblob.name} ({myblob.length} bytes)\")\n",
    "\n",
    "    session = Session(engine)\n",
    "\n",
    "    try:\n",
    "        # Create question extractor with custom config\n",
    "        factory = ExtractorFactory()\n",
    "        config = load_config()\n",
    "        question_extractor = factory.create_question_extractor(\n",
    "            strategy=EXTRACTION_STRATEGY,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "\n",
    "        logging.info(f\"[Questions] Using extraction strategy: {EXTRACTION_STRATEGY}\")\n",
    "\n",
    "        # For context-aware strategy, inject structure extractor\n",
    "        if EXTRACTION_STRATEGY == \"context_aware\":\n",
    "            # Structure extractor needs its own config (not QuestionExtractorConfig)\n",
    "            structure_extractor = factory.create_structure_extractor(\n",
    "                strategy=\"structure_aware\"\n",
    "                # Uses default StructureExtractorConfig with LLM settings from env\n",
    "            )\n",
    "            question_extractor.set_structure_extractor(structure_extractor)\n",
    "            logging.info(\"[Questions] Structure extractor injected for context-aware extraction\")\n",
    "\n",
    "        # Wrap question extractor in adapter to make it compatible with DocumentProcessor\n",
    "        # The adapter converts ExtractedQuestion â†’ ExtractedQAPair with answer_text=None\n",
    "        qa_extractor = QuestionToQAAdapter(question_extractor)\n",
    "\n",
    "        # Create parser (defaults to env var DOCUMENT_PARSER)\n",
    "        parser = create_parser()\n",
    "\n",
    "        # Use DocumentProcessor for all orchestration and database logic\n",
    "        processor = DocumentProcessor(\n",
    "            session=session,\n",
    "            parser=parser,\n",
    "            extractor=qa_extractor\n",
    "        )\n",
    "\n",
    "        # Process document (handles parsing, extraction, and database persistence)\n",
    "        result = processor.process_document(\n",
    "            file_content=file_bytes,\n",
    "            filename=\"myblob.name\",\n",
    "            client_name=\"Unknown Client\",\n",
    "            source_type=\"Blank RFP\"\n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            logging.info(f\"[Questions] Success: {result.message}\")\n",
    "        else:\n",
    "            logging.warning(f\"[Questions] Processing completed with issues: {result.message}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        logging.error(f\"[Questions] Error processing {'myblob.name'}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "\n",
    "# Loading PDF bytes with Pymu/Fitz\n",
    "pdf_filepath_ = \"/home/dougsgrott-wsl/projects/local-doc-extractor/data/RFP - Email Service Provider.pdf\"\n",
    "doc = fitz.open(pdf_filepath_)\n",
    "pdf_bytes = doc.tobytes()\n",
    "\n",
    "main(pdf_bytes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57265252",
   "metadata": {},
   "source": [
    "## Question and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bca3688b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 13:20:40,770 - INFO - [Q&A] Processing file.\n",
      "2025-12-29 13:20:40,777 - INFO - [Q&A] Using extraction strategy: context_aware\n",
      "2025-12-29 13:20:40,782 - INFO - [Q&A] Structure extractor injected for context-aware extraction\n",
      "2025-12-29 13:20:40,782 - INFO - Processing file: myblob.name\n",
      "2025-12-29 13:20:40,783 - INFO - Step 1: Extracting text from PDF...\n",
      "2025-12-29 13:20:42,519 - INFO - Extracted 30023 characters of text (format: markdown)\n",
      "2025-12-29 13:20:42,519 - INFO - Step 2: Identifying Questions and Answers with LLM...\n",
      "2025-12-29 13:20:42,520 - INFO - Starting context-aware Q+A extraction...\n",
      "2025-12-29 13:20:42,520 - INFO - Detecting document structure for answer extraction...\n",
      "2025-12-29 13:20:42,521 - INFO - Starting AI-powered structure detection...\n",
      "2025-12-29 13:20:42,521 - INFO - Created 5 strategic samples\n",
      "2025-12-29 13:20:46,147 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:20:46,366 - INFO - Format detected: formal_numbered (confidence: 0.95)\n",
      "2025-12-29 13:21:10,504 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:21:10,720 - INFO - Detected 32 sections\n",
      "2025-12-29 13:21:10,726 - INFO - Structure detection complete. Sections: 30\n",
      "2025-12-29 13:21:10,735 - INFO - Detecting document structure...\n",
      "2025-12-29 13:21:10,735 - INFO - Starting AI-powered structure detection...\n",
      "2025-12-29 13:21:10,737 - INFO - Created 5 strategic samples\n",
      "2025-12-29 13:21:15,166 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:21:15,381 - INFO - Format detected: formal_numbered (confidence: 0.95)\n",
      "2025-12-29 13:21:44,608 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:21:44,827 - INFO - Detected 32 sections\n",
      "2025-12-29 13:21:44,830 - INFO - Structure detection complete. Sections: 30\n",
      "2025-12-29 13:21:44,831 - INFO - Extracting with structure awareness (30 sections)\n",
      "2025-12-29 13:21:49,737 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:21:52,261 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:00,388 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:06,334 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:06,343 - INFO - After deduplication: 47 questions\n",
      "2025-12-29 13:22:06,344 - INFO - Extracted 47 questions, now attempting to find answers...\n",
      "2025-12-29 13:22:09,497 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:10,829 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:11,877 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:12,679 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:13,509 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:14,522 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:15,467 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:16,150 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:18,157 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:19,052 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:19,940 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:20,805 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:21,933 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:22,828 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:23,595 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:24,280 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:25,146 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:25,926 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:26,596 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:27,475 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:28,449 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:29,477 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:30,277 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:30,939 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:31,659 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:32,328 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:33,139 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:33,855 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:34,569 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:35,143 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:33,748 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:34,592 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:35,402 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:36,102 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:36,796 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:37,482 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:38,411 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:39,312 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:40,172 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:40,872 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:41,555 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:42,414 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:43,169 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:43,810 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:44,478 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:45,066 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:45,925 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-29 13:22:45,950 - INFO - Produced 47 Q+A pairs\n",
      "2025-12-29 13:22:45,951 - INFO - Extracted 47 potential Q&A pairs\n",
      "2025-12-29 13:22:45,951 - INFO - Step 3: Persisting to PostgreSQL...\n",
      "2025-12-29 13:22:46,519 - INFO - Successfully committed 47 Q&A pairs to DB\n",
      "2025-12-29 13:22:46,520 - INFO - [Q&A] Success: Successfully processed 47 Q&A pairs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import azure.functions as func\n",
    "import logging\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session\n",
    "import fitz\n",
    "\n",
    "from config import config\n",
    "from shared.core.processor import DocumentProcessor\n",
    "from shared.extractors import ExtractorFactory\n",
    "from shared.extractors.base.config import QAExtractorConfig\n",
    "from shared.parsers import create_parser\n",
    "\n",
    "app = func.FunctionApp()\n",
    "\n",
    "# Database Connection Setup\n",
    "engine = create_engine(config.database_url)\n",
    "\n",
    "# Extraction Strategy Configuration\n",
    "EXTRACTION_STRATEGY = os.getenv(\"EXTRACTION_STRATEGY\", \"simple\")\n",
    "# Options: simple (fast, basic), context_aware (better quality, structure-aware)\n",
    "\n",
    "\n",
    "def load_config() -> QAExtractorConfig:\n",
    "    \"\"\"Load Q&A extraction configuration from environment variables.\"\"\"\n",
    "    return QAExtractorConfig(\n",
    "        chunk_size=int(os.getenv(\"CHUNK_SIZE\", \"8000\")),\n",
    "        overlap=int(os.getenv(\"CHUNK_OVERLAP\", \"500\")),\n",
    "        confidence_threshold=float(os.getenv(\"CONFIDENCE_THRESHOLD\", \"0.5\")),\n",
    "        deduplicate_pairs=os.getenv(\"ENABLE_DEDUP\", \"true\").lower() == \"true\",\n",
    "        similarity_threshold=float(os.getenv(\"SIMILARITY_THRESHOLD\", \"0.9\")),\n",
    "        # LLM config auto-loaded from base ExtractorConfig\n",
    "    )\n",
    "\n",
    "\n",
    "def main(file_bytes):\n",
    "    \"\"\"\n",
    "    1. Parses the PDF document (using Azure DI or pymupdf)\n",
    "    2. Extracts question-answer pairs using LLM\n",
    "    3. Saves both questions and answers to the database\n",
    "    \"\"\"\n",
    "    logging.info(f\"[Q&A] Processing file.\")\n",
    "\n",
    "    session = Session(engine)\n",
    "\n",
    "    try:\n",
    "        # Create extractor with custom config\n",
    "        factory = ExtractorFactory()\n",
    "        config = load_config()\n",
    "        extractor = factory.create_qa_extractor(strategy=EXTRACTION_STRATEGY, config=config)\n",
    "\n",
    "        logging.info(f\"[Q&A] Using extraction strategy: {EXTRACTION_STRATEGY}\")\n",
    "\n",
    "        # For context-aware strategy, inject structure extractor\n",
    "        if EXTRACTION_STRATEGY == \"context_aware\":\n",
    "            # Structure extractor needs its own config (not QAExtractorConfig)\n",
    "            structure_extractor = factory.create_structure_extractor(\n",
    "                strategy=\"structure_aware\"\n",
    "                # Uses default StructureExtractorConfig with LLM settings from env\n",
    "            )\n",
    "            # Context-aware QA extractor uses structure extractor internally\n",
    "            extractor.set_structure_extractor(structure_extractor)\n",
    "            logging.info(\"[Q&A] Structure extractor injected for context-aware extraction\")\n",
    "\n",
    "        # Create parser (defaults to env var DOCUMENT_PARSER)\n",
    "        parser = create_parser()\n",
    "\n",
    "        # Process using core processor\n",
    "        processor = DocumentProcessor(session, parser=parser, extractor=extractor)\n",
    "        result = processor.process_document(\n",
    "            file_content=file_bytes,\n",
    "            filename=\"myblob.name\",\n",
    "            client_name=\"Unknown Client\",\n",
    "            source_type=\"Q&A Document\"\n",
    "        )\n",
    "\n",
    "        # Log result\n",
    "        if result.success:\n",
    "            logging.info(f\"[Q&A] Success: {result.message}\")\n",
    "        else:\n",
    "            logging.error(f\"[Q&A] Failed: {result.message}\")\n",
    "            if result.error:\n",
    "                raise result.error\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        logging.error(f\"[Q&A] Error processing {'myblob.name'}: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "\n",
    "# Loading PDF bytes with Pymu/Fitz\n",
    "pdf_filepath_ = \"/home/dougsgrott-wsl/projects/local-doc-extractor/data/RFP - Email Service Provider.pdf\"\n",
    "doc = fitz.open(pdf_filepath_)\n",
    "pdf_bytes = doc.tobytes()\n",
    "\n",
    "main(pdf_bytes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-doc-extractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
